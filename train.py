# -*- coding: utf-8 -*-
"""project 2 - Phoneme Classification

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-aWHOMa0DClaMfdNVOce4ILDNkFHEtsh


"""## Training"""

#check device
def get_device():
  return "cuda" if torch.cuda.is_available() else "cpu"

#fix random seed
def same_seed(seed):
  if torch.cuda.is_available():
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) 
  else:
    torch.manual_seed(seed)
  np.random.seed(seed)
  torch.backends.cudnn.benchmark = False #do not need to spend extra time to search best convolution algorithm in each layer
  torch.backends.cudnn.determinstic = True #fix algorithm avoid output different

#parameter setting
same_seed(42)
device = get_device()
print("device:", device)

config = {
    'num_epoch': 20,                # maximum number of epochs
    'optimizer': 'Adam',              # optimization algorithm (optimizer in torch.optim)
    'optim_hparas': {                # hyper-parameters for the optimizer (depends on which optimizer you are using)
        'lr': 0.0001,                 # learning rate of SGD
    },
    'early_stop': 200,               # early stopping epochs (the number epochs since your model's last improvement)
    'model_path': './model.ckpt'  #model check point save path
}

#create model, loss function, optimizer
model = Classifier().to(device)
criteria = nn.CrossEntropyLoss()
#optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)

def training(train_loader, val_loader, tr_set, val_set, model, config, device, criteria):
  best_acc = 0.0
  n_epochs = config['num_epoch']
  optimizer = getattr(torch.optim, config['optimizer'])(model.parameters(), **config['optim_hparas'])
  early_stop_count = 0
  for epoch in range(n_epochs):
    train_acc = 0.0
    train_loss = 0.0
    model.train()
    for i, data in enumerate(train_loader):
      x, y = data
      x, y = x.to(device), y.to(device)
      optimizer.zero_grad()
      pred = model(x)
      batch_loss = criteria(pred, y)
      _, train_pred = torch.max(pred, 1)
      batch_loss.backward()
      optimizer.step()
      train_acc += (train_pred.cpu() == y.cpu()).sum().item()
      train_loss += batch_loss.item()
    #early stop
    val_acc, val_loss = validation(val_loader, model, device, criteria)
    if val_acc > best_acc:
      best_acc = val_acc
      torch.save(model.state_dict(), config["model_path"])
      print("saving model (epoch = {:4d}, loss = {:4f})".format(epoch +1, best_acc))
      early_stop_cnt = 0
    else:
      early_stop_cnt += 1
    epoch += 1
    if early_stop_cnt > config["early_stop"]:
    # Stop training if your model stops improving for "config['early_stop']" epochs.
      break
    print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(epoch, n_epochs, train_acc/len(tr_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)))
  print('Finished training after {} epochs'.format(epoch))
  return best_acc, train_acc, train_loss, val_acc, val_loss

"""## Validation"""

def validation(val_loader, model, device, criteria):
  model.eval()
  val_acc = 0
  val_loss = 0
  with torch.no_grad():
    for i, data in enumerate(val_loader):
      x, y = data
      x, y = x.to(device), y.to(device)
      pred = model(x)
      batch_loss = criteria(pred, y)
      _, val_pred = torch.max(pred, 1)
      val_acc += (val_pred.cpu() == y.cpu()).sum().item()
      val_loss += batch_loss.item()
  return val_acc, val_loss

"""## test"""

test_set = TIMITDataset(test, None)
test_loader = DataLoader(test_set, batch_size = BATCH_SIZE, shuffle = True)

def testing(test_loader, model, device, criteria):
  #load last best model check point parameters
  model = Classifier().to(device)
  model.load_state_dict(torch.load(config["model_path"]))
  prediction = []
  model.eval()
  with torch.no_grad():
    for i, data in enumerate(test_loader):
      x = data
      x = x.to(device)
      pred = model(x)
      _, test_pred = torch.max(pred, 1)
      for item in test_pred.cpu().numpy():
        prediction.append(item)
  return prediction

"""## strat training"""

best_acc, train_acc, train_loss, val_acc, val_loss = training(train_loader, val_loader, train_set, val_set, model, config, device, criteria)

"""## prediction"""

prediction = testing(test_loader, model, device, criteria)
with open("prediction.csv", "w") as f:
  f.write("index, class\n")
  for index, label in enumerate(prediction):
    f.write("{}, {}\n".format(index, label))