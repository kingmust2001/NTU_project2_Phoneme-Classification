# -*- coding: utf-8 -*-
"""project 2 - Phoneme Classification

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-aWHOMa0DClaMfdNVOce4ILDNkFHEtsh

## Download Data
Download data from google drive, then unzip it.

You should have `timit_11/train_11.npy`, `timit_11/train_label_11.npy`, and `timit_11/test_11.npy` after running this block.<br><br>
`timit_11/`
- `train_11.npy`: training data<br>
- `train_label_11.npy`: training label<br>
- `test_11.npy`:  testing data<br><br>
"""

!gdown --id '1HPkcmQmFGu-3OknddKIa5dNDsR05lIQR' --output data.zip
!unzip data.zip
!ls

#import os
#os.getcwd()

"""## import package

"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
#from sklearn.model_selection import train_test_split

"""## Preparing Data
Load the training and testing data from the `.npy` file (NumPy array).
"""

import numpy as np
print("Loading data...")

data_root = "./timit_11/"
train = np.load(data_root + "train_11.npy")
train_label = np.load(data_root + "train_label_11.npy")
test = np.load(data_root + "test_11.npy")

print(" size of training dataset:", train.shape, "\n", "size of testing dataset:", test.shape)

#train[:10,::]

"""## create Dataset

"""

class TIMITDataset(Dataset):
    def __init__(self, X, y=None):
        self.data = torch.from_numpy(X).float()
        if y is not None:
            y = y.astype(np.int)
            self.label = torch.LongTensor(y)
        else:
            self.label = None

    def __getitem__(self, idx):
        if self.label is not None:
            return self.data[idx], self.label[idx]
        else:
            return self.data[idx]

    def __len__(self):
        return len(self.data)

"""## split data"""

"""VAL_RATIO = 0.2
train_x, val_x, train_y, val_y = train_test_split(train, train_label, test_size = VAL_RATIO, random_state=42)
print('Size of training set: {}'.format(train_x.shape))
print('Size of validation set: {}'.format(val_x.shape))"""

VAL_RATIO = 0.2

percent = int(train.shape[0] * (1 - VAL_RATIO))
train_x, train_y, val_x, val_y = train[:percent], train_label[:percent], train[percent:], train_label[percent:]
print('Size of training set: {}'.format(train_x.shape))
print('Size of validation set: {}'.format(val_x.shape))

"""## create Dataloader"""

BATCH_SIZE = 64

train_set = TIMITDataset(train_x, train_y)
val_set = TIMITDataset(val_x, val_y)
train_loader = DataLoader(train_set, batch_size = BATCH_SIZE, shuffle = True) # only shuffle training set
val_loader = DataLoader(val_set, batch_size = BATCH_SIZE, shuffle=False)

"""## clean memory

Cleanup the unneeded variables to save memory.

notes: if you need to use these variables later, then you may remove this block or clean up unneeded variables later
the data size is quite huge, so be aware of memory usage in colab
"""

import gc

del train, train_label, train_x, train_y, val_x, val_y
gc.collect()

"""## Model

"""

"""class Classifier(nn.Module):
  def __init__(self):
    super(Classifier, self).__init__()
    self.net = nn.Sequential(
        nn.Linear(429, 1024),
        nn.Sigmoid(),
        nn.Linear(1024, 512),
        nn.Sigmoid(),
        nn.Linear(512, 128),
        nn.Sigmoid(),
        nn.Linear(128, 39))
  def forward(self, x):
    return self.net(x)"""


class Classifier(nn.Module):
    def __init__(self):
        super(Classifier, self).__init__()
        self.layer1 = nn.Linear(429, 1024)
        self.layer2 = nn.Linear(1024, 512)
        self.layer3 = nn.Linear(512, 128)
        self.out = nn.Linear(128, 39) 

        self.act_fn = nn.Sigmoid()

    def forward(self, x):
        x = self.layer1(x)
        x = self.act_fn(x)

        x = self.layer2(x)
        x = self.act_fn(x)

        x = self.layer3(x)
        x = self.act_fn(x)

        x = self.out(x)
        
        return x

print(Classifier())

"""## Training"""

#check device
def get_device():
  return "cuda" if torch.cuda.is_available() else "cpu"

#fix random seed
def same_seed(seed):
  if torch.cuda.is_available():
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) 
  else:
    torch.manual_seed(seed)
  np.random.seed(seed)
  torch.backends.cudnn.benchmark = False #do not need to spend extra time to search best convolution algorithm in each layer
  torch.backends.cudnn.determinstic = True #fix algorithm avoid output different

#parameter setting
same_seed(42)
device = get_device()
print("device:", device)

config = {
    'num_epoch': 20,                # maximum number of epochs
    'optimizer': 'Adam',              # optimization algorithm (optimizer in torch.optim)
    'optim_hparas': {                # hyper-parameters for the optimizer (depends on which optimizer you are using)
        'lr': 0.0001,                 # learning rate of SGD
    },
    'early_stop': 200,               # early stopping epochs (the number epochs since your model's last improvement)
    'model_path': './model.ckpt'  #model check point save path
}

#create model, loss function, optimizer
model = Classifier().to(device)
criteria = nn.CrossEntropyLoss()
#optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)

def training(train_loader, val_loader, tr_set, val_set, model, config, device, criteria):
  best_acc = 0.0
  n_epochs = config['num_epoch']
  optimizer = getattr(torch.optim, config['optimizer'])(model.parameters(), **config['optim_hparas'])
  early_stop_count = 0
  for epoch in range(n_epochs):
    train_acc = 0.0
    train_loss = 0.0
    model.train()
    for i, data in enumerate(train_loader):
      x, y = data
      x, y = x.to(device), y.to(device)
      optimizer.zero_grad()
      pred = model(x)
      batch_loss = criteria(pred, y)
      _, train_pred = torch.max(pred, 1)
      batch_loss.backward()
      optimizer.step()
      train_acc += (train_pred.cpu() == y.cpu()).sum().item()
      train_loss += batch_loss.item()
    #early stop
    val_acc, val_loss = validation(val_loader, model, device, criteria)
    if val_acc > best_acc:
      best_acc = val_acc
      torch.save(model.state_dict(), config["model_path"])
      print("saving model (epoch = {:4d}, loss = {:4f})".format(epoch +1, best_acc))
      early_stop_cnt = 0
    else:
      early_stop_cnt += 1
    epoch += 1
    if early_stop_cnt > config["early_stop"]:
    # Stop training if your model stops improving for "config['early_stop']" epochs.
      break
    print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f} | Val Acc: {:3.6f} loss: {:3.6f}'.format(epoch, n_epochs, train_acc/len(tr_set), train_loss/len(train_loader), val_acc/len(val_set), val_loss/len(val_loader)))
  print('Finished training after {} epochs'.format(epoch))
  return best_acc, train_acc, train_loss, val_acc, val_loss

"""## Validation"""

def validation(val_loader, model, device, criteria):
  model.eval()
  val_acc = 0
  val_loss = 0
  with torch.no_grad():
    for i, data in enumerate(val_loader):
      x, y = data
      x, y = x.to(device), y.to(device)
      pred = model(x)
      batch_loss = criteria(pred, y)
      _, val_pred = torch.max(pred, 1)
      val_acc += (val_pred.cpu() == y.cpu()).sum().item()
      val_loss += batch_loss.item()
  return val_acc, val_loss

"""## test"""

test_set = TIMITDataset(test, None)
test_loader = DataLoader(test_set, batch_size = BATCH_SIZE, shuffle = True)

def testing(test_loader, model, device, criteria):
  #load last best model check point parameters
  model = Classifier().to(device)
  model.load_state_dict(torch.load(config["model_path"]))
  prediction = []
  model.eval()
  with torch.no_grad():
    for i, data in enumerate(test_loader):
      x = data
      x = x.to(device)
      pred = model(x)
      _, test_pred = torch.max(pred, 1)
      for item in test_pred.cpu().numpy():
        prediction.append(item)
  return prediction

"""## strat training"""

best_acc, train_acc, train_loss, val_acc, val_loss = training(train_loader, val_loader, train_set, val_set, model, config, device, criteria)

"""## prediction"""

prediction = testing(test_loader, model, device, criteria)
with open("prediction.csv", "w") as f:
  f.write("index, class\n")
  for index, label in enumerate(prediction):
    f.write("{}, {}\n".format(index, label))